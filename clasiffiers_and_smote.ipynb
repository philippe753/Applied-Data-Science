{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clasiffiers and smote.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMeXPTvPJq4GJHYUGhIN17M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philippe753/Applied-Data-Science/blob/main/clasiffiers_and_smote.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sZ0A6TO9YmtI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import norm, skew\n",
        "from scipy.special import boxcox1p\n",
        "from scipy.stats import boxcox_normmax\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score \n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import Ridge, Lasso, LogisticRegression\n",
        "# Loading the data\n",
        "df = pd.read_csv('gdrive/MyDrive/Colab Notebooks/creditcard.csv')\n",
        "# df = pd.read_csv('./data/creditcard.csv')\n",
        "# Checking the class distribution of the target variable\n",
        "df['Class'].value_counts()\n",
        "# Checking the class distribution of the target variable in percentage\n",
        "print((df.groupby('Class')['Class'].count()/df['Class'].count()) *100)\n",
        "((df.groupby('Class')['Class'].count()/df['Class'].count()) *100).plot.pie()\n",
        "# Checking the correlation\n",
        "corr = df.corr()\n",
        "corr\n",
        "# Checking the correlation in heatmap\n",
        "plt.figure(figsize=(24,18))\n",
        "\n",
        "sns.heatmap(corr, cmap=\"coolwarm\", annot=True)\n",
        "plt.show()\n",
        "# Checking the % distribution of normal vs fraud\n",
        "classes=df['Class'].value_counts()\n",
        "normal_share=classes[0]/df['Class'].count()*100\n",
        "fraud_share=classes[1]/df['Class'].count()*100\n",
        "\n",
        "print(normal_share)\n",
        "print(fraud_share)\n",
        "# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.countplot(df['Class'])\n",
        "plt.title(\"Class Count\", fontsize=18)\n",
        "plt.xlabel(\"Record counts by class\", fontsize=15)\n",
        "plt.ylabel(\"Count\", fontsize=15)\n",
        "plt.show()\n",
        "# Splitting the dataset into X and y\n",
        "y= df['Class']\n",
        "X = df.drop(['Class'], axis=1)\n",
        "# Splitting the dataset using train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100, test_size=0.20)\n",
        "# Accumulating all the column names under one variable\n",
        "cols = list(X.columns.values)\n",
        "#Create a dataframe to store results\n",
        "df_Results = pd.DataFrame(columns=['Methodology','Model','Accuracy','roc_value','threshold'])\n",
        "# Created a common function to plot confusion matrix\n",
        "def Plot_confusion_matrix(y_test, pred_test):\n",
        "  cm = confusion_matrix(y_test, pred_test)\n",
        "  plt.clf()\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Accent)\n",
        "  categoryNames = ['Non-Fraudalent','Fraudalent']\n",
        "  plt.title('Confusion Matrix - Test Data')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  ticks = np.arange(len(categoryNames))\n",
        "  plt.xticks(ticks, categoryNames, rotation=45)\n",
        "  plt.yticks(ticks, categoryNames)\n",
        "  s = [['TN','FP'], ['FN', 'TP']]\n",
        "  \n",
        "  for i in range(2):\n",
        "      for j in range(2):\n",
        "          plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),fontsize=12)\n",
        "  plt.show()\n",
        "  # # Created a common function to fit and predict on a Logistic Regression model for both L1 and L2\n",
        "def buildAndRunLogisticModels(df_Results, Methodology, X_train,y_train, X_test, y_test ):\n",
        "\n",
        "  # Logistic Regression\n",
        "  from sklearn import linear_model\n",
        "  from sklearn.model_selection import KFold\n",
        "\n",
        "  num_C = list(np.power(10.0, np.arange(-10, 10)))\n",
        "  cv_num = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "  searchCV_l2 = linear_model.LogisticRegressionCV(\n",
        "          Cs= num_C\n",
        "          ,penalty='l2'\n",
        "          ,scoring='roc_auc'\n",
        "          ,cv=cv_num\n",
        "          ,random_state=42\n",
        "          ,max_iter=10000\n",
        "          ,fit_intercept=True\n",
        "          ,solver='newton-cg'\n",
        "          ,tol=10\n",
        "      )\n",
        "\n",
        "  searchCV_l1 = linear_model.LogisticRegressionCV(\n",
        "          Cs=num_C\n",
        "          ,penalty='l1'\n",
        "          ,scoring='roc_auc'\n",
        "          ,cv=cv_num\n",
        "          ,random_state=42\n",
        "          ,max_iter=10000\n",
        "          ,fit_intercept=True\n",
        "          ,solver='liblinear'\n",
        "          ,tol=10\n",
        "      )\n",
        "\n",
        "  searchCV_l1.fit(X_train, y_train)\n",
        "  searchCV_l2.fit(X_train, y_train)\n",
        "  print ('Max auc_roc for l1:', searchCV_l1.scores_[1].mean(axis=0).max())\n",
        "  print ('Max auc_roc for l2:', searchCV_l2.scores_[1].mean(axis=0).max())\n",
        "\n",
        "  print(\"Parameters for l1 regularisations\")\n",
        "  print(searchCV_l1.coef_)\n",
        "  print(searchCV_l1.intercept_) \n",
        "  print(searchCV_l1.scores_)\n",
        "\n",
        "  print(\"Parameters for l2 regularisations\")\n",
        "  print(searchCV_l2.coef_)\n",
        "  print(searchCV_l2.intercept_) \n",
        "  print(searchCV_l2.scores_)  \n",
        "\n",
        "\n",
        "  #find predicted vallues\n",
        "  y_pred_l1 = searchCV_l1.predict(X_test)\n",
        "  y_pred_l2 = searchCV_l2.predict(X_test)\n",
        "  \n",
        "\n",
        "  #Find predicted probabilities\n",
        "  y_pred_probs_l1 = searchCV_l1.predict_proba(X_test)[:,1] \n",
        "  y_pred_probs_l2 = searchCV_l2.predict_proba(X_test)[:,1] \n",
        "\n",
        "  # Accuaracy of L2/L1 models\n",
        "  Accuracy_l2 = metrics.accuracy_score(y_pred=y_pred_l2, y_true=y_test)\n",
        "  Accuracy_l1 = metrics.accuracy_score(y_pred=y_pred_l1, y_true=y_test)\n",
        "\n",
        "  print(\"Accuarcy of Logistic model with l2 regularisation : {0}\".format(Accuracy_l2))\n",
        "  print(\"Confusion Matrix\")\n",
        "  Plot_confusion_matrix(y_test, y_pred_l2)\n",
        "  print(\"classification Report\")\n",
        "  print(classification_report(y_test, y_pred_l2))\n",
        "    \n",
        "  print(\"Accuarcy of Logistic model with l1 regularisation : {0}\".format(Accuracy_l1))\n",
        "  print(\"Confusion Matrix\")\n",
        "  Plot_confusion_matrix(y_test, y_pred_l1)\n",
        "  print(\"classification Report\")\n",
        "  print(classification_report(y_test, y_pred_l1))\n",
        "\n",
        "  l2_roc_value = roc_auc_score(y_test, y_pred_probs_l2)\n",
        "  print(\"l2 roc_value: {0}\" .format(l2_roc_value))\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_probs_l2)\n",
        "  threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "  print(\"l2 threshold: {0}\".format(threshold))\n",
        "\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  print(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\n",
        "  plt.plot(fpr,tpr,label=\"Test, auc=\"+str(roc_auc))\n",
        "  plt.legend(loc=4)\n",
        "  plt.show()\n",
        "  \n",
        "  df_Results = df_Results.append(pd.DataFrame({'Methodology': Methodology,'Model': 'Logistic Regression with L2 Regularisation','Accuracy': Accuracy_l2,'roc_value': l2_roc_value,'threshold': threshold}, index=[0]),ignore_index= True)\n",
        "\n",
        "  l1_roc_value = roc_auc_score(y_test, y_pred_probs_l1)\n",
        "  print(\"l1 roc_value: {0}\" .format(l1_roc_value))\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_probs_l1)\n",
        "  threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "  print(\"l1 threshold: {0}\".format(threshold))\n",
        "\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  print(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\n",
        "  plt.plot(fpr,tpr,label=\"Test, auc=\"+str(roc_auc))\n",
        "  plt.legend(loc=4)\n",
        "  plt.show()\n",
        "  \n",
        "  df_Results = df_Results.append(pd.DataFrame({'Methodology': Methodology,'Model': 'Logistic Regression with L1 Regularisation','Accuracy': Accuracy_l1,'roc_value': l1_roc_value,'threshold': threshold}, index=[0]),ignore_index= True)\n",
        "  return df_Results\n",
        "  # Created a common function to fit and predict on a Random Forest model\n",
        "def buildAndRunRandomForestModels(df_Results, Methodology, X_train,y_train, X_test, y_test ):\n",
        "  #Evaluate Random Forest model\n",
        "\n",
        "  # Create the model with 100 trees\n",
        "  RF_model = RandomForestClassifier(n_estimators=100, \n",
        "                                bootstrap = True,\n",
        "                                max_features = 'sqrt', random_state=42)\n",
        "  # Fit on training data\n",
        "  RF_model.fit(X_train, y_train)\n",
        "  RF_test_score = RF_model.score(X_test, y_test)\n",
        "  RF_model.predict(X_test)\n",
        "\n",
        "  print('Model Accuracy: {0}'.format(RF_test_score))\n",
        "\n",
        "\n",
        "  # Actual class predictions\n",
        "  rf_predictions = RF_model.predict(X_test)\n",
        "\n",
        "  print(\"Confusion Matrix\")\n",
        "  Plot_confusion_matrix(y_test, rf_predictions)\n",
        "  print(\"classification Report\")\n",
        "  print(classification_report(y_test, rf_predictions))\n",
        "\n",
        "  # Probabilities for each class\n",
        "  rf_probs = RF_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "  # Calculate roc auc\n",
        "  roc_value = roc_auc_score(y_test, rf_probs)\n",
        "\n",
        "  print(\"Random Forest roc_value: {0}\" .format(roc_value))\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y_test, rf_probs)\n",
        "  threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "  print(\"Random Forest threshold: {0}\".format(threshold))\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  print(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\n",
        "  plt.plot(fpr,tpr,label=\"Test, auc=\"+str(roc_auc))\n",
        "  plt.legend(loc=4)\n",
        "  plt.show()\n",
        "  \n",
        "  df_Results = df_Results.append(pd.DataFrame({'Methodology': Methodology,'Model': 'Random Forest','Accuracy': RF_test_score,'roc_value': roc_value,'threshold': threshold}, index=[0]),ignore_index= True)\n",
        "\n",
        "  return df_Results\n",
        "  #Lets perfrom StratifiedKFold and check the results\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
        "# X is the feature set and y is the target\n",
        "for train_index, test_index in skf.split(X,y):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train_SKF_cv, X_test_SKF_cv = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_SKF_cv, y_test_SKF_cv = y.iloc[train_index], y.iloc[test_index]\n",
        "    #Run Logistic Regression with L1 And L2 Regularisation\n",
        "print(\"Logistic Regression with L1 And L2 Regularisation\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunLogisticModels(df_Results,\"StratifiedKFold Cross Validation\", X_train_SKF_cv,y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*60 )\n",
        "#Run Random Forest Model\n",
        "print(\"Random Forest Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunRandomForestModels(df_Results,\"StratifiedKFold Cross Validation\",X_train_SKF_cv,y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*60 )\n",
        "# Creating dataframe with Smote and StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn import over_sampling\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X,y), 1):\n",
        "    X_train = X.loc[train_index]\n",
        "    y_train = y.loc[train_index] \n",
        "    X_test = X.loc[test_index]\n",
        "    y_test = y.loc[test_index]  \n",
        "    SMOTE = over_sampling.SMOTE(random_state=0)\n",
        "    X_train_Smote, y_train_Smote= SMOTE.fit_resample(X_train, y_train)\n",
        "  \n",
        "X_train_Smote = pd.DataFrame(data=X_train_Smote,   columns=cols)\n",
        "Data_Imbalance_Handiling\t = \"SMOTE Oversampling with StratifiedKFold CV \"\n",
        "#Run Logistic Regression with L1 And L2 Regularisation\n",
        "print(\"Logistic Regression with L1 And L2 Regularisation\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunLogisticModels(df_Results, Data_Imbalance_Handiling, X_train_Smote, y_train_Smote , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run Random Forest Model\n",
        "print(\"Random Forest Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunRandomForestModels(df_Results, Data_Imbalance_Handiling, X_train_Smote, y_train_Smote , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "df_Results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "df = pd.read_csv('gdrive/MyDrive/Colab Notebooks/creditcard.csv')\n",
        "# df = pd.read_csv('./data/creditcard.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "D7pKW3wKYsh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i3PAGYXdF9sY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}